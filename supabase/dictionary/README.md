# Supabase Dictionary Import Tools

Enhanced tools for importing Chinese-Vietnamese dictionary data into Supabase with proper role management and environment configuration.

## ï¿½ Quick Setup

1. **Install dependencies:**
   ```bash
   npm install
   ```

2. **Setup environment:**
   ```bash
   npm run setup  # Creates .env from template
   nano .env      # Edit with your Supabase credentials
   ```

3. **Generate SQL files:**
   ```bash
   npm run import:bulk  # Process all JSON files in ./import/
   ```

4. **Deploy to Supabase:**
   ```bash
   npm run deploy  # Full deployment with role management
   ```

## ğŸ” Environment Configuration

Create a `.env` file with your Supabase settings:

```bash
# Required for Node.js deployment (deploy-node.js)
SUPABASE_PROJECT_ID=your-project-id
SUPABASE_DB_PASSWORD=your-database-password

# Required for JavaScript client (insert_dict_data.js)
SUPABASE_URL=https://your-project-id.supabase.co
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key
SUPABASE_ANON_KEY=your-anon-key
```

### How to get these values:

1. Go to [Supabase Dashboard](https://app.supabase.com)
2. Select your project
3. **For API keys:** Settings â†’ API
   - Copy Project URL â†’ `SUPABASE_URL`
   - Copy anon/public key â†’ `SUPABASE_ANON_KEY`
   - Copy service_role key â†’ `SUPABASE_SERVICE_ROLE_KEY` (keep secret!)
4. **For database:** Settings â†’ Database
   - Copy Project ID from URL or Host field
   - Password is what you set during project creation 
       "type": "Ä‘áº¡i tá»«",
       "meaning_vi": "ChÃºng (dÃ¹ng Ä‘á»ƒ chá»‰ Ä‘á»™ng váº­t hoáº·c Ä‘á»“ váº­t)",
       "meaning_en": "They/them (used to refer to animals or objects)",
       "example_cn": "æˆ‘å–œæ¬¢è¿™äº›å°ç‹—ï¼Œå®ƒä»¬å¾ˆå¯çˆ±ã€‚",
       "example_vi": "TÃ´i thÃ­ch nhá»¯ng chÃº chÃ³ con nÃ y, chÃºng ráº¥t Ä‘Ã¡ng yÃªu.",
       "example_en": "I like these puppies; they are very cute.",
       "grammar": "Äáº¡i tá»« sá»‘ nhiá»u, dÃ¹ng Ä‘á»ƒ chá»‰ cÃ¡c sá»± váº­t hoáº·c Ä‘á»™ng váº­t khÃ´ng pháº£i lÃ  ngÆ°á»i."
     }
   ]
   ```

## ğŸ”§ Usage

### Basic Import
```bash
node dictionayImport.js your_data.json
```

### Advanced Usage
```bash
# Test with sample data
node dictionayImport.js test_data.json

# Import from qwen output
node dictionayImport.js ../qwen/dictionary/output/dict-processed-process-1.json

# Check help
node dictionayImport.js
```

### Import to Database
```bash
# After generating SQL file
psql -d your_database -f output/dictionary_insert.sql

# Or for Supabase (using connection string)
psql "postgresql://user:pass@host:port/dbname" -f output/dictionary_insert.sql
```

## ğŸ“Š Data Validation

The script validates:
- âœ… Required fields (chinese, pinyin, meanings, examples)
- âœ… Data types and non-empty values
- âœ… Field length constraints
- âœ… Word type normalization

**Supported Word Types**:
- `danh tá»«` (noun)
- `Ä‘á»™ng tá»«` (verb) 
- `tÃ­nh tá»«` (adjective)
- `tráº¡ng tá»«` (adverb)
- `Ä‘áº¡i tá»«` (pronoun)
- `giá»›i tá»«` (preposition)
- `liÃªn tá»«` (conjunction)
- `thÃ¡n tá»«` (interjection)
- `sá»‘ tá»«` (numeral)
- `lÆ°á»£ng tá»«` (classifier)
- `phÃ³ tá»«` (adverb)
- `other` (fallback)

## ğŸ¯ Performance Optimization

### Database Indexes
- **GIN Indexes**: For trigram fuzzy search on text fields
- **Composite Indexes**: For common query patterns
- **tsvector**: For full-text search across all fields

### Import Performance
- **Batch Size**: 1000 records per INSERT (configurable)
- **Transaction Management**: Single transaction for consistency
- **Memory Efficiency**: Streams large files without loading entire dataset

### Search Performance
- **Fuzzy Matching**: Handles typos in Chinese characters and Vietnamese text
- **Accent Handling**: Vietnamese search works with/without accents
- **Ranked Results**: Results sorted by similarity score

## ğŸ“ˆ Monitoring & Statistics

### Built-in Views
```sql
-- Overall statistics
SELECT * FROM dictionary_stats;

-- Performance monitoring  
SELECT * FROM dictionary_performance_stats;
```

### Search Functions
```sql
-- Vietnamese search
SELECT * FROM search_dictionary_vietnamese('yÃªu thÆ°Æ¡ng') LIMIT 10;

-- Chinese search  
SELECT * FROM search_dictionary_chinese('çˆ±') LIMIT 10;
```

## ğŸ—‚ï¸ File Structure

```
dictionary/
â”œâ”€â”€ dictionayImport.js          # Main import script
â”œâ”€â”€ test_data.json              # Sample data for testing
â”œâ”€â”€ package.json                # Node.js configuration
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ output/                     # Generated SQL files
â”‚   â”œâ”€â”€ dictionary_insert.sql   # Main import SQL
â”‚   â””â”€â”€ import_log.txt          # Validation errors (if any)
â””â”€â”€ ../db/
    â””â”€â”€ dictionary_schema.sql   # Database schema
```

## ğŸš¨ Troubleshooting

### Common Issues

**1. "column has pseudo-type anyarray"**
- âœ… **Fixed**: Schema updated to cast `most_common_vals` properly

**2. "enum value does not exist"**  
- Check word type mapping in script
- Use 'other' for unknown types

**3. "JSON parse error"**
- Validate JSON format with `jq` or online validator
- Check for trailing commas or unescaped quotes

**4. "Out of memory"**
- Reduce batch size in script configuration  
- Process data in smaller chunks

### Performance Issues

**Slow Import (12K+ records)**:
- Increase `work_mem` and `maintenance_work_mem` in PostgreSQL
- Use SSD storage
- Disable triggers temporarily during import
- Consider using `COPY` for very large datasets

**Slow Search**:
- Ensure all indexes are created
- Run `ANALYZE dictionary;` after import
- Check query plans with `EXPLAIN ANALYZE`

## ğŸ“‹ Example Output

```
ğŸš€ Starting dictionary import...
ğŸ“– Reading input file: test_data.json
ğŸ“Š Loaded 12000 entries
ğŸ” Validating entries...
âœ… Valid entries: 11950
âŒ Invalid entries: 50
ğŸ”§ Generating SQL statements...
ğŸ“¦ Processing 11950 entries in 12 batches...
âœ“ Generated batch 1/12 (1000 records)
...
âœ“ Generated batch 12/12 (950 records)
âœ… Import SQL generated successfully!
ğŸ“ Output file: output/dictionary_insert.sql
ğŸ“ˆ File size: 12.45 MB

ğŸ“Š Import Summary:
   Total entries: 12000
   Valid entries: 11950  
   Invalid entries: 50
   Validation errors: 127
   SQL batches: 12
   Success rate: 99.6%
```

## ğŸ”® Future Enhancements

- [ ] Support for CSV input format
- [ ] Incremental import (update existing records)
- [ ] Audio file integration for pronunciation
- [ ] HSK level classification
- [ ] API endpoint for real-time import
- [ ] Docker containerization

## ğŸ“ License

MIT License - see project root for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch
3. Test with sample data
4. Submit pull request

For questions or issues, please create an issue in the repository.
